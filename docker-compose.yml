version: '3.8'

services:
  # FastAPI application
  api:
    build: .
    container_name: simplecrawl-api
    ports:
      - "8000:8000"
    environment:
      - HOST=0.0.0.0
      - PORT=8000
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite:////app/data/simplecrawl.db
      - MEDIA_STORAGE_DIR=/app/media
      - LOG_LEVEL=INFO
      - HEADLESS=true
      # Crawl limits (increased for deep crawling)
      - MAX_CRAWL_DEPTH=50
      - MAX_CRAWL_PAGES=5000
      # Proxy settings (uncomment and configure to enable)
      # - PROXY_ROTATION_ENABLED=true
      # - PROXY_URL=http://user:pass@proxy:8080
      # - PROXY_LIST_FILE=/app/proxies.txt
      # - PROXY_ROTATION_STRATEGY=round_robin
      # - PROXY_MAX_FAILURES=3
      # - PROXY_COOLDOWN_SECONDS=300
    depends_on:
      - redis
    volumes:
      - ./app:/app/app
      - ./data:/app/data
      - ./media:/app/media
      # Mount proxy list file if using file-based proxies
      # - ./proxies.txt:/app/proxies.txt:ro
    restart: unless-stopped
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  # Celery worker for async jobs
  worker:
    build: .
    container_name: simplecrawl-worker
    environment:
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite:////app/data/simplecrawl.db
      - MEDIA_STORAGE_DIR=/app/media
      - LOG_LEVEL=INFO
      - HEADLESS=true
      # Crawl limits (must match api service)
      - MAX_CRAWL_DEPTH=50
      - MAX_CRAWL_PAGES=5000
      # Proxy settings (uncomment and configure to enable)
      # - PROXY_ROTATION_ENABLED=true
      # - PROXY_URL=http://user:pass@proxy:8080
      # - PROXY_LIST_FILE=/app/proxies.txt
      # - PROXY_ROTATION_STRATEGY=round_robin
      # - PROXY_MAX_FAILURES=3
      # - PROXY_COOLDOWN_SECONDS=300
    depends_on:
      - redis
    volumes:
      - ./app:/app/app
      - ./data:/app/data
      - ./media:/app/media
      # Mount proxy list file if using file-based proxies
      # - ./proxies.txt:/app/proxies.txt:ro
    restart: unless-stopped
    command: celery -A app.workers.tasks worker --loglevel=info --concurrency=4

  # Celery Beat scheduler for periodic tasks (monitoring, cleanup)
  beat:
    build: .
    container_name: simplecrawl-beat
    environment:
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite:////app/data/simplecrawl.db
      - LOG_LEVEL=INFO
      # Job retention (hours) - jobs older than this are auto-cleaned
      - JOB_RETENTION_HOURS=24
    depends_on:
      - redis
      - worker
    volumes:
      - ./app:/app/app
      - ./data:/app/data
    restart: unless-stopped
    command: celery -A app.workers.tasks beat --loglevel=info

  # Redis for queue and caching
  redis:
    image: redis:7-alpine
    container_name: simplecrawl-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

volumes:
  redis-data:
